{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WIP Exterior Accessibility Classifier",
      "provenance": [],
      "collapsed_sections": [
        "5F_HkHLM71qM"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Noznoc/access-classifier/blob/main/WIP_Exterior_Accessibility_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMmkRAEj71qE"
      },
      "source": [
        "# CNN Binary Classifier Building Enterances\n",
        "**Author:** Alex Lane and Julia Conzon <br>\n",
        "**Date created:** 2021/03/18<br>\n",
        "**Description:** Training an image classifier from scratch on the Mapillary API imagery collected on building enterances. This workflow follows the Classifier from [Scratch Keras tutorial](https://keras.io/examples/vision/image_classification_from_scratch/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUlNem0w71qL"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdgPeOIliLQ5"
      },
      "source": [
        "Import the necessary Python packages for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqoxWghw71qL"
      },
      "source": [
        "import sys # for importing directory\n",
        "import tensorflow as tf # for modeling\n",
        "from tensorflow import keras # for modeling\n",
        "from keras.models import Sequential # following Wu et al. (2019) sequential model was used\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Activation, BatchNormalization\n",
        "import matplotlib.pyplot as plt # for visualizing images\n",
        "import numpy as np"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F_HkHLM71qM"
      },
      "source": [
        "## Load the data: \"Accessible\" and \"Inaccessible\" dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP3GRSXf71qM"
      },
      "source": [
        "Compile data from the Mapillary API, [scripts accessible in another notebook]()\n",
        "\n",
        "Confirm folder of Mapillary API imagery exists. Subfolders should be `Accessible` and `Inaccessible`, which each contain the images that represent the category.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdJXPFCU71qN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33be6c9e-bfc8-4c2b-f912-8f4ac07b8b59"
      },
      "source": [
        "!ls OSM_And_Mapillary_Sequence_Dataset\n",
        "\n",
        "import os, random\n",
        "import shutil\n",
        "\n",
        "# Function that randomly moves image files based on source directory, destination directory and number of files wanting to move\n",
        "def randomly_move_imgs(src_dir, dst_dir, n):\n",
        "  file_list = os.listdir(src_dir)\n",
        "  for j in range(n):\n",
        "    img = random.choice(file_list)\n",
        "    shutil.move(src_dir + img, dst_dir)\n",
        "\n",
        "# Read \"OSM_And_Mapillary_Sequence_Dataset/Accessible\" folder randomly select n and move to \"Test/Accessible\" folder\n",
        "n = 10\n",
        "src_dir = \"./OSM_And_Mapillary_Sequence_Dataset/Train/Accessible/\"\n",
        "dst_dir = \"./OSM_And_Mapillary_Sequence_Dataset/Test/Accessible\"\n",
        "# randomly_move_imgs(src_dir, dst_dir, n)\n",
        "\n",
        "# Read \"OSM_And_Mapillary_Sequence_Dataset/Inaccessible\" folder and randomly select n and move to \"Test/Inaccessible\" folder\n",
        "n = 10\n",
        "src_dir = \"./OSM_And_Mapillary_Sequence_Dataset/Train/Inaccessible/\"\n",
        "dst_dir = \"./OSM_And_Mapillary_Sequence_Dataset/Test/Inaccessible\"\n",
        "# randomly_move_imgs(src_dir, dst_dir, n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model  Test  Train\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQh8O9sJ71qN"
      },
      "source": [
        "## Preprocess Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6YszFjEoL99"
      },
      "source": [
        "Steps includes standardizing the images to the same size (`image_size`). \n",
        "\n",
        "Keras `seed` command is an optional parameter to offer a random seed for shuffling and transforming the images to augment the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsr2kd5g71qN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed2a289-c7be-441f-dc08-ec64732ef4b3"
      },
      "source": [
        "image_size = (512, 512)\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "# Assign to a train dataset (85%)\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"OSM_And_Mapillary_Sequence_Dataset/Train\",\n",
        "    validation_split=0.15,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode = 'binary',\n",
        ")\n",
        "\n",
        "# Assign to a validation dataset (15%)\n",
        "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"OSM_And_Mapillary_Sequence_Dataset/Train\",\n",
        "    validation_split=0.15,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    label_mode = 'binary',\n",
        ")\n",
        "\n",
        "# confirm classes\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 824 files belonging to 2 classes.\n",
            "Using 701 files for training.\n",
            "Found 824 files belonging to 2 classes.\n",
            "Using 123 files for validation.\n",
            "['Accessible', 'Inaccessible']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1KTVYuq71qO"
      },
      "source": [
        "## Visualize Data\n",
        "\n",
        "Label 1 is \"inaccessible\" and label 0 is \"accessible\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxCTfcGi71qO"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POS4yzwL71qO"
      },
      "source": [
        "## Augmentation\n",
        "\n",
        "Considering the small sample for training, the dataset will be augmented, that is randomly flipped horizontally, rotated, contrasted and zoomed. Order impacts the image gets distorted. Different tests were applied to identify an optimal augmentation, as seen below:\n",
        "\n",
        "https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W_iOd5P71qO"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        # Reminder, the order in which transformations are called significantly \n",
        "        # affects the final augmentation.\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomTranslation(0.1,0.1),\n",
        "        layers.experimental.preprocessing.RandomZoom(0.2),\n",
        "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "        layers.experimental.preprocessing.RandomContrast(0.01),\n",
        "        #layers.experimental.preprocessing.RandomHeight(0.5),\n",
        "        #layers.experimental.preprocessing.RandomWidth(0.5),\n",
        "        #layers.experimental.preprocessing.Resizing(180, 180),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP9OSBZo71qO"
      },
      "source": [
        "Visualize the augmented samples:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4bovMy_71qP"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVOSjwxK71qP"
      },
      "source": [
        "## Configure Dataset\n",
        "\n",
        "Following documentation from Keras creator, François Chollet, use buffered prefetching to yield data from disk without having I/O becoming blocking:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7nqXAqg71qQ"
      },
      "source": [
        "train_ds = train_ds.prefetch(buffer_size=32)\n",
        "valid_ds = valid_ds.prefetch(buffer_size=32)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbvb13xa71qQ"
      },
      "source": [
        "## Build Our Model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA63DYE75hjH"
      },
      "source": [
        "# MODEL ARCHITECTURES BY WU ET AL (2019):\n",
        "\n",
        "# CNN2: CONV(64, 3), RELU, CONV(32, 3), RELU, POOL(2), CONV(64, 3), FC(128), RELU, FC(2), SOFTMAX.\n",
        "# CNN3: [CONV(128, 3), RELU, POOL(2)], [CONV(64 3), RELU, POOL(2)], [CONV(32, 3), RELU, POOL(2)], DROPOUT(), RELU, FC(512), RELU, FC(2), SOFTMAX.\n",
        "# CNN5: CONV(128, 3), RELU, CONV(128, 3), RELU, POOL(2), CONV(64, 3), DROPOUT(), RELU, CONV(64, 3), RELU, POOL(2), CONV(32, 3), DROPOUT(), RELU, POOL(2), FC(512), RELU, DROPOUT(), FC(256), RELU, FC(2), SOFTMAX\n",
        "\n",
        "# Model parameters\n",
        "input_shape = image_size + (3,) # add RGB code of 3 to image shape for Keras Input() argument\n",
        "num_classes = 2 # number of classes being defined\n",
        "\n",
        "# Initialize model\n",
        "inputs = keras.Input(shape=input_shape) # establish inputs e.g., (180, 180, 3) is the width and height dimension and 3 represents RGB\n",
        "print(inputs)\n",
        "layer = data_augmentation(inputs) # first \"layer\" is augmented images that are called in the previous script\n",
        "\n",
        "# rescale the image to 1 and 0, not 255 RGB\n",
        "layer = layers.experimental.preprocessing.Rescaling(1.0 / 255)(layer)\n",
        "\n",
        "# Building blocks for CNN\n",
        "\n",
        "layer = Conv2D(filters=16, kernel_size=3)(layer)\n",
        "layer = Activation('relu')(layer)\n",
        "layer = MaxPooling2D(pool_size = (2,2))(layer)\n",
        "\n",
        "layer = Conv2D(filters=32, kernel_size=3, activation='relu')(layer)\n",
        "layer = MaxPooling2D(pool_size = (2,2))(layer)\n",
        "\n",
        "layer = Conv2D(filters=64, kernel_size=3, activation='relu')(layer)\n",
        "layer = MaxPooling2D(pool_size = (2,2))(layer)\n",
        "\n",
        "layer = Conv2D(filters=64, kernel_size=3, activation='relu')(layer)\n",
        "layer = MaxPooling2D(pool_size = (2,2))(layer)\n",
        "\n",
        "layer = Conv2D(filters=128, kernel_size=3, activation='relu')(layer)\n",
        "layer = MaxPooling2D(pool_size = (2,2))(layer)\n",
        "\n",
        "layer = Flatten()(layer)\n",
        "\n",
        "layer = Dense(512, activation='relu')(layer)\n",
        "outputs = Dense(1, activation='sigmoid')(layer) # or use sigmoid to view probability distribution\n",
        "\n",
        "print(outputs)\n",
        "\n",
        "# Initialize model with building blocks\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Plot model architecture\n",
        "keras.utils.plot_model(model, to_file='model.png', rankdir='TR', show_shapes='True')\n",
        "\n",
        "# Print summary of model\n",
        "# model.summary()\n",
        "\n",
        "# Print each layer\n",
        "# for layer in model.layers:\n",
        "#   print(layer.output_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18T1cgLjHaN8"
      },
      "source": [
        "# Build Wu CNN2 Model - WILL CRASH\n",
        "Start off with a simple Sequential model, following Wu et al. (2019)\n",
        "Their CNNs require way too much memory, OutOfMemoryError, we'd need a better GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV7PXa2_HeZo"
      },
      "source": [
        "# MODEL ARCHITECTURES BY WU ET AL (2019):\n",
        "\n",
        "# CNN2: CONV(64, 3), RELU, CONV(32, 3), RELU, POOL(2), CONV(64, 3), FC(128), RELU, FC(2), SOFTMAX.\n",
        "# CNN3: [CONV(128, 3), RELU, POOL(2)], [CONV(64 3), RELU, POOL(2)], [CONV(32, 3), RELU, POOL(2)], DROPOUT(), RELU, FC(512), RELU, FC(2), SOFTMAX.\n",
        "# CNN5: CONV(128, 3), RELU, CONV(128, 3), RELU, POOL(2), CONV(64, 3), DROPOUT(), RELU, CONV(64, 3), RELU, POOL(2), CONV(32, 3), DROPOUT(), RELU, POOL(2), FC(512), RELU, DROPOUT(), FC(256), RELU, FC(2), SOFTMAX\n",
        "\n",
        "# Model parameters\n",
        "input_shape = image_size + (3,) # add RGB code of 3 to image shape for Keras Input() argument\n",
        "num_classes = 2 # number of classes being defined\n",
        "\n",
        "# Initialize model\n",
        "inputs = keras.Input(shape=input_shape) # establish inputs e.g., (180, 180, 3) is the width and height dimension and 3 represents RGB\n",
        "print(inputs)\n",
        "layer = data_augmentation(inputs) # first \"layer\" is augmented images that are called in the previous script\n",
        "\n",
        "# rescale the image to 1 and 0, not 255 RGB\n",
        "layer = layers.experimental.preprocessing.Rescaling(1.0 / 255)(layer)\n",
        "\n",
        "# Building blocks for CNN\n",
        "\n",
        "layer = Conv2D(filters=64, kernel_size=3, activation='relu')(layer)\n",
        "layer = MaxPooling2D(pool_size = (2,2))(layer)\n",
        "\n",
        "layer = Conv2D(filters=32, kernel_size=3, activation='relu')(layer)\n",
        "layer = MaxPooling2D(pool_size = (2,2))(layer)\n",
        "\n",
        "layer = Flatten()(layer)\n",
        "\n",
        "layer = Dense(128, activation='relu')(layer)\n",
        "\n",
        "outputs = Dense(2, activation='softmax')(layer) # or use sigmoid to view probability distribution\n",
        "\n",
        "print(outputs)\n",
        "\n",
        "# Initialize model with building blocks\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Plot model architecture\n",
        "keras.utils.plot_model(model, to_file='model.png', rankdir='TR', show_shapes='True')\n",
        "\n",
        "# Print summary of model\n",
        "# model.summary()\n",
        "\n",
        "# Print each layer\n",
        "# for layer in model.layers:\n",
        "#   print(layer.output_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3JbSE0H71qR"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QouopGep71qR"
      },
      "source": [
        "# Number of epochs\n",
        "epochs = 50\n",
        "\n",
        "# Add callbacks to log file\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"OSM_And_Mapillary_Sequence_Data/Model/Epochs/{epoch}.h5\"),\n",
        "]\n",
        "\n",
        "# Compile CNN, following default optimizer and loss function for binary classification\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "classifier = model.fit(train_ds, epochs=epochs, callbacks=callbacks, validation_data=valid_ds, shuffle=True)\n",
        "\n",
        "# This following visualization is taken from \n",
        "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
        "# summarize history for accuracy\n",
        "plt.plot(classifier.history['accuracy'])\n",
        "plt.plot(classifier.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(classifier.history['loss'])\n",
        "plt.plot(classifier.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwtfFGwsZriZ"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZOJ-mkzZy-v"
      },
      "source": [
        "model.save('./OSM_And_Mapillary_Sequence_Dataset/Model/Saved Models/50 Epoch Low Val Acc/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYxKIGChZwA9"
      },
      "source": [
        "# Load Model\n",
        "\n",
        "Alternatively, load a model instead of training a new one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr2GvsTaZzRz"
      },
      "source": [
        "model = keras.models.load_model('./OSM_And_Mapillary_Sequence_Dataset/Model/Saved Models/100 Epoch Overfit/')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukS0mwZB71qR"
      },
      "source": [
        "# Test Model\n",
        "\n",
        "I think the scores were opposite, so I changed the code somewhat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksIKNn2f71qR"
      },
      "source": [
        "import os\n",
        "\n",
        "directory = r'./OSM_And_Mapillary_Sequence_Dataset/Test/Accessible/'\n",
        "correctInaccessible = 0\n",
        "correctAccessible = 0\n",
        "\n",
        "for testImage in os.listdir(directory):\n",
        "    if testImage.endswith(\".png\"):\n",
        "        #print(os.path.join(directory, testImage))\n",
        "        img = keras.preprocessing.image.load_img(os.path.join(directory, testImage), \n",
        "                                                 target_size=image_size)\n",
        "        img_array = keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = tf.expand_dims(img_array, 0)  # create batch axis\n",
        "\n",
        "        predictions = model.predict(img_array)\n",
        "        score = predictions[0]\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.pause(0.1)\n",
        "        print(testImage)\n",
        "        print(\n",
        "            \"This image of building entrance is %.2f percent accessible and %.2f percent inaccessible.\"\n",
        "            % (100 * (1 - score), 100 * score)\n",
        "        )\n",
        "        if((1 - score) > 0.5):\n",
        "            print(\"✅ - This image has been correctly classified as accessible.\\n\")\n",
        "            correctAccessible += 1\n",
        "        elif ((1 - score) == 0.5):\n",
        "            print(\"❓ - The model say this image has equal probability of being accesible or inaccessible.\\n\")\n",
        "        else:\n",
        "            print(\"❌ - This image has been incorrectly classified as inaccessible.\\n\")\n",
        "\n",
        "directory = r'./OSM_And_Mapillary_Sequence_Dataset/Test/Inaccessible/'\n",
        "for testImage in os.listdir(directory):\n",
        "    if testImage.endswith(\".png\"):\n",
        "        #print(os.path.join(directory, testImage))\n",
        "        img = keras.preprocessing.image.load_img(os.path.join(directory, testImage), \n",
        "                                                 target_size=image_size)\n",
        "        img_array = keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = tf.expand_dims(img_array, 0)  # create batch axis\n",
        "\n",
        "        predictions = model.predict(img_array)\n",
        "        score = predictions[0]\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.pause(0.1)\n",
        "        print(testImage)\n",
        "        print(\n",
        "            \"This image of building entrance is %.2f percent accessible and %.2f percent inaccessible.\"\n",
        "            % (100 * (1 - score), 100 * score)\n",
        "        )\n",
        "        if(score > 0.5):\n",
        "            print(\"✅ - This image has been correctly classified as inaccessible.\\n\")\n",
        "            correctInaccessible += 1\n",
        "        elif (score == 0.5):\n",
        "            print(\"❓ - The model say this image has equal probability of being accesible or inaccessible.\\n\")\n",
        "        else:\n",
        "            print(\"❌ - This image has been incorrectly classified as accessible.\\n\")\n",
        "\n",
        "print(\"%i of the 10 accessible images were correctly classified as accessible.\" % correctAccessible)\n",
        "print(\"%i of the 10 inaccessible images were correctly classified as inaccessible.\" % correctInaccessible)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}